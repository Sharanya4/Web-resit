{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Question 1 Ans \n",
    "#first import the packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import ggplot\n",
    "import seaborn as sns\n",
    "#added the library\n",
    "working_dir = os.getcwd() + ('/code')\n",
    "sys.path.append(working_dir)\n",
    "from B_Data_Preprocessing import *\n",
    "#get the data first from outer source\n",
    "train_dataset_dataset = pd.read_csv('./data/train_dataset_dataset.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "validation = pd.read_csv('./data/validation.csv')\n",
    "#Aalysis the given data\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "#data Analysis.\n",
    "train_dataset_dataset.shape\n",
    "train_dataset_dataset.describe().transpose()\n",
    "train_dataset_dataset.info()\n",
    "\n",
    "# Set the commom values\n",
    "train_dataset[\"advertiser\"].value_counts()\n",
    "train_dataset[\"click\"].value_counts() \n",
    "#find out the clicks.\n",
    "\n",
    "data.groupby('opsys', {'CTR':data.aggregate.average('click')})\n",
    "\n",
    "# find the common values in columns\n",
    "train_dataset.T.apply(lambda x: x.nunique(), axis=1)\n",
    "\n",
    "## Data Analysis by  Advertiser.\n",
    "train_dataset_by_advertiser = pd.DataFrame({'impressions': train_dataset.groupby('advertiser').size()}).reset_index() \n",
    "# find out the Find the impression values of the dataset.\n",
    "train_dataset_by_advertiser = train_dataset_by_advertiser.join(pd.DataFrame({'clicks': train_dataset[train_dataset['click'] == 1].groupby('advertiser').size()}).reset_index(drop=True)) \n",
    "# calculate the all clicks\n",
    "train_dataset_by_advertiser['CTR'] = (train_dataset_by_advertiser['clicks']/train_dataset_by_advertiser['impressions']*100).round(4)\n",
    " # Click-Through Rate\n",
    "train_dataset_by_advertiser = (train_dataset_by_advertiser.join(pd.DataFrame({'cost': train_dataset.groupby(['advertiser'])['payprice'].sum()}).reset_index(drop=True)/1000)) \n",
    "# find the coste values\n",
    "train_dataset_by_advertiser['CPM'] = (train_dataset_by_advertiser['cost']*1000/train_dataset_by_advertiser['impressions']).round(4) \n",
    "# FInd out the CPM values\n",
    "train_dataset_by_advertiser['eCPC'] = (train_dataset_by_advertiser['cost']/train_dataset_by_advertiser['clicks']).round(4) \n",
    "# Find out the CTC\n",
    "train_dataset_by_advertiser['']\n",
    "train_dataset_by_advertiser.to_latex()\n",
    "#day by day to analysis the dataset.\n",
    "train_dataset_by_weekday = pd.DataFrame({'impressions': train_dataset.groupby('weekday').size()}).reset_index() \n",
    "# check the impression.\n",
    "train_dataset_by_weekday = (train_dataset_by_weekday.join(pd.DataFrame({'clicks': train_dataset[train_dataset['click'] == 1].groupby('weekday').size()}).reset_index(drop=True))) \n",
    "# calculate the all clicks\n",
    "train_dataset_by_weekday['CTR'] = (train_dataset_by_weekday['clicks']/train_dataset_by_weekday['impressions']*100) \n",
    "# Click-Through Rate\n",
    "train_dataset_by_weekday = (train_dataset_by_weekday.join(pd.DataFrame({'cost': train_dataset.groupby(['weekday'])['payprice'].sum()}).reset_index(drop=True)/1000)) # find the coste values\n",
    "train_dataset_by_weekday['CPM'] = (train_dataset_by_weekday['cost']*1000/train_dataset_by_weekday['impressions']).round(2) \n",
    "# FInd out the CPM values\n",
    "train_dataset_by_weekday['eCPC'] = (train_dataset_by_weekday['cost']/train_dataset_by_weekday['clicks']).round(2) \n",
    "# Find out the CTC\n",
    "train_dataset_by_weekday.to_latex()\n",
    "\n",
    "CTR_vs_Weekday = train_dataset[['click', 'bidprice', 'payprice', 'advertiser', 'weekday']].groupby(\n",
    "    ['advertiser', 'weekday']).sum()\n",
    "CTR_vs_Weekday['impressions'] = train_dataset.groupby(['advertiser', 'weekday']).size()\n",
    "CTR_vs_Weekday['CTR'] = CTR_vs_Weekday['click'] / CTR_vs_Weekday['impressions']\n",
    "Plot_CTR_vs_Weekday = CTR_vs_Weekday.unstack('advertiser').loc[:, 'impressions'][[2997, 3358, 1458]]\n",
    "Plot_CTR_vs_Weekday.fillna(0.0, inplace=True) \n",
    " # Fill Nans with zero\n",
    "Plot_CTR_vs_Weekday.plot(kind=\"line\", color=['royalblue', 'darkred', 'darkgreen']) \n",
    " # kind=\"bar\"\n",
    "ax = plt.gca()\n",
    "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "plt.xlabel('Weekday', fontsize=8)\n",
    "plt.ylabel('Impressions', fontsize=8)\n",
    "plt.xticks(rotation=0)\n",
    "plt.title('Impressions Comparison based on Day of the Week', fontsize=10)\n",
    "plt.legend(fontsize=6)\n",
    "plt.tick_params(labelsize=6)\n",
    "plt.show()\n",
    "plt.savefig(os.getcwd()+'/results/line_hourly.pdf')\n",
    "\n",
    "\n",
    "## ANALYSIS BY HOUR\n",
    "train_dataset_by_hour = pd.DataFrame({'impressions': train_dataset.groupby('hour').size()}).reset_index() \n",
    "# Find the impression values\n",
    "train_dataset_by_hour = train_dataset_by_hour.join(pd.DataFrame({'clicks': train_dataset[train_dataset['click'] == 1].groupby('hour').size()}).reset_index(drop=True)) # calculate the all clicks\n",
    "train_dataset_by_hour['CTR'] = train_dataset_by_hour['clicks']/train_dataset_by_hour['impressions']*100 \n",
    "# Click-Through Rate\n",
    "train_dataset_by_hour = train_dataset_by_hour.join(pd.DataFrame({'cost': train_dataset.groupby(['hour'])['payprice'].sum()}).reset_index(drop=True)/1000) # find the coste values\n",
    "train_dataset_by_hour['CPM'] = train_dataset_by_hour['cost']*1000/train_dataset_by_hour['impressions'] \n",
    "# FInd out the CPM values\n",
    "train_dataset_by_hour['eCPC'] = train_dataset_by_hour['cost']/train_dataset_by_hour['clicks'] \n",
    "# Find out the CTC\n",
    "\n",
    "CTR_vs_Hour = train_dataset[['click', 'bidprice', 'payprice', 'advertiser', 'hour']].groupby(['advertiser', 'hour']).sum()\n",
    "CTR_vs_Hour['impressions'] = train_dataset.groupby(['advertiser', 'hour']).size()\n",
    "CTR_vs_Hour['CTR'] = CTR_vs_Hour['click'] / CTR_vs_Hour['impressions']\n",
    "Plot_CTR_vs_Hour = CTR_vs_Hour.unstack('advertiser').loc[:, 'click'][[2997, 3358, 1458]]\n",
    "Plot_CTR_vs_Hour.fillna(0.0, inplace=True)  \n",
    "# Enter the non-zero rows\n",
    "Plot_CTR_vs_Hour.plot(kind=\"line\", color=['royalblue', 'darkred', 'forestgreen']) \n",
    " # kind=\"bar\"\n",
    "plt.xlabel('Hour', fontsize=8)\n",
    "plt.ylabel('Clicks', fontsize=8)\n",
    "plt.title('Clicks Comparison based on Hour of the Day', fontsize=10)\n",
    "plt.legend(fontsize=6)\n",
    "plt.tick_params(labelsize=6)\n",
    "plt.show()\n",
    "plt.savefig(os.getcwd()+'/results/line_clicks.pdf')\n",
    "\n",
    "\n",
    "#Analysis the operating system.\n",
    "train_dataset = separate_useragent(train_dataset)\n",
    "train_dataset_by_op_sys = pd.DataFrame({'impressions': train_dataset.groupby('opsys').size()}).reset_index() \n",
    "# Find the impression values\n",
    "train_dataset_by_op_sys = train_dataset_by_op_sys.join(pd.DataFrame({'clicks': train_dataset[train_dataset['click'] == 1].groupby('opsys').size()}).reset_index(drop=True)) \n",
    "# calculate the all clicks\n",
    "train_dataset_by_op_sys['CTR'] = train_dataset_by_op_sys['clicks']/train_dataset_by_op_sys['impressions']*100 \n",
    "# Find out the rates\n",
    "train_dataset_by_op_sys = train_dataset_by_op_sys.join(pd.DataFrame({'cost': train_dataset.groupby(['hour'])['payprice'].sum()}).reset_index(drop=True)/1000) \n",
    "# find the coste values\n",
    "train_dataset_by_op_sys['CPM'] = train_dataset_by_op_sys['cost']*1000/train_dataset_by_op_sys['impressions'] \n",
    "# FInd out the CPM values\n",
    "train_dataset_by_op_sys['eCPC'] = train_dataset_by_op_sys['cost']/train_dataset_by_op_sys['clicks'] \n",
    "# Find out the CTC\n",
    "\n",
    "CTR_vs_OS = train_dataset[['click', 'bidprice', 'payprice', 'advertiser', 'opsys']].groupby(['advertiser', 'opsys']).sum()\n",
    "CTR_vs_OS['impressions'] = train_dataset.groupby(['advertiser', 'opsys']).size()\n",
    "CTR_vs_OS['CTR'] = CTR_vs_OS['click'] / CTR_vs_OS['impressions']\n",
    "Plot_CTR_vs_OS = CTR_vs_OS.unstack('advertiser').loc[:, 'CTR'][[2997, 3358, 1458, 2259]]\n",
    "Plot_CTR_vs_OS.fillna(0.0, inplace=True) \n",
    "Plot_CTR_vs_OS.plot(kind=\"bar\", color=['royalblue', 'darkred', 'forestgreen', 'darkorange'])  \n",
    "plt.xlabel('`Operating System', fontsize=8)\n",
    "plt.ylabel('CTR', fontsize=8)\n",
    "plt.xticks(rotation=0)\n",
    "plt.title('CTR Comparison based on Operating System', fontsize=10)\n",
    "plt.legend(fontsize=6)\n",
    "plt.tick_params(labelsize=6)\n",
    "plt.savefig(os.getcwd()+'/results/barplot.pdf')\n",
    "\n",
    "\n",
    "## Data analysis by Browser\n",
    "train_dataset = separate_useragent(train_dataset)\n",
    "train_dataset_by_browser = pd.DataFrame({'impressions': train_dataset.groupby('browser').size()}).reset_index() \n",
    "# Find the impression values\n",
    "train_dataset_by_browser = train_dataset_by_browser.join(pd.DataFrame({'clicks': train_dataset[train_dataset['click'] == 1].groupby('browser').size()}).reset_index(drop=True)) \n",
    "# calculate the all clicks\n",
    "train_dataset_by_browser['CTR'] = train_dataset_by_browser['clicks']/train_dataset_by_browser['impressions']*100 \n",
    "# Click-Through Rate\n",
    "train_dataset_by_browser = train_dataset_by_browser.join(pd.DataFrame({'cost': train_dataset.groupby(['hour'])['payprice'].sum()}).reset_index(drop=True)/1000)\n",
    " # find the coste values\n",
    "train_dataset_by_browser['CPM'] = train_dataset_by_browser['cost']*1000/train_dataset_by_browser['impressions'] \n",
    "# FInd out the CPM values\n",
    "train_dataset_by_browser['eCPC'] = train_dataset_by_browser['cost']/train_dataset_by_browser['clicks'] \n",
    "\n",
    "# Data Slot analysis\n",
    "train_dataset = slot_width_height_combiner(train_dataset)\n",
    "train_dataset_by_slot = pd.DataFrame({'impressions': train_dataset.groupby('slot_width_height').size()}).reset_index() \n",
    "# Find the impression values\n",
    "train_dataset_by_slot = train_dataset_by_slot.join(pd.DataFrame({'clicks': train_dataset[train_dataset['click'] == 1].groupby('slot_width_height').size()}).reset_index(drop=True)) \n",
    "# calculate the all clicks\n",
    "train_dataset_by_slot['CTR'] = train_dataset_by_slot['clicks']/train_dataset_by_slot['impressions']*100 \n",
    "# Click-Through Rate\n",
    "train_dataset_by_slot = train_dataset_by_slot.join(pd.DataFrame({'cost': train_dataset.groupby(['hour'])['payprice'].sum()}).reset_index(drop=True)/1000) \n",
    "# find the coste values\n",
    "train_dataset_by_slot['CPM'] = train_dataset_by_slot['cost']*1000/train_dataset_by_slot['impressions']\n",
    " # FInd out the CPM values\n",
    "train_dataset_by_slot['eCPC'] = train_dataset_by_slot['cost']/train_dataset_by_slot['clicks']\n",
    " # Find out CTC\n",
    "train_dataset['slotprice_binned'] = train_dataset['slotprice'].apply(slot_price_bucketing)\n",
    "train_dataset_by_slotprice = pd.DataFrame({'impressions': train_dataset.groupby('slotprice_binned').size()}).reset_index()\n",
    " # Find the total Impression.\n",
    "train_dataset_by_slotprice = train_dataset_by_slotprice.join(pd.DataFrame({'clicks': train_dataset[train_dataset['click'] == 1].groupby('slotprice_binned').size()}).reset_index(drop=True)) \n",
    "# calculate the all clicks\n",
    "train_dataset_by_slotprice['CTR'] = train_dataset_by_slotprice['clicks']/train_dataset_by_slotprice['impressions']*100\n",
    " # Click-Through Rate\n",
    "train_dataset_by_slotprice = train_dataset_by_slotprice.join(pd.DataFrame({'cost': train_dataset.groupby(['hour'])['payprice'].sum()}).reset_index(drop=True)/1000) \n",
    "# find the coste values\n",
    "train_dataset_by_slotprice['CPM'] = train_dataset_by_slotprice['cost']*1000/train_dataset_by_slotprice['impressions'] \n",
    "# FInd out the CPM values\n",
    "train_dataset_by_slotprice['eCPC'] = train_dataset_by_slotprice['cost']/train_dataset_by_slotprice['clicks'] \n",
    "# clicks should be analysis\n",
    "train_dataset_only_clicks = train_dataset[train_dataset['click'] == 1]\n",
    "train_dataset_only_clicks.groupby('weekday').size()\n",
    "train_dataset_only_clicks.groupby('hour').size()\n",
    "train_dataset_only_clicks.groupby('advertiser').size()\n",
    "train_dataset_only_clicks.groupby('weekday').size()\n",
    "\n",
    "#Anlysis the prise\n",
    "\n",
    "n, bins, patches = plt.hist(train_dataset['bidprice'],  50, normed=1, facecolor='green', alpha=0.25, label='Paid Price')\n",
    "n, bins, patches = plt.hist(validation['bidprice'], 50, normed=1, facecolor='red', alpha=0.25, label='Bid Price')\n",
    "n, bins, patches = plt.hist(train_dataset['slotprice'], 50, normed=1, facecolor='blue', alpha=0.75, label='Slot (Floor) Price')\n",
    "\n",
    "plt.xlabel('Price (Chinese Fen)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of Prices')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show() \n",
    "\n",
    "# Draw the histogram\n",
    "n, bins, patches = plt.hist(train_dataset['bidprice']-train_dataset['payprice'],  50, normed=1, facecolor='green', alpha=0.75)\n",
    "\n",
    "plt.xlabel('Price (Chinese Fen)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of Difference between Bid Price and Paid Price')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "#plot the values.\n",
    "plt.show() \n",
    "\n",
    "df_all = np.array(train_dataset[['bidprice', 'payprice', 'slotprice']])\n",
    "train_dataset_clicks = train_dataset[train_dataset['click'] == 1]\n",
    "df_clicks = np.array(train_dataset_clicks[['bidprice', 'payprice', 'slotprice']])\n",
    "\n",
    "\n",
    "# Create some ploting function\n",
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "\n",
    "#using this function to find out the axis style\n",
    "def set_axis_style(ax, labels):\n",
    "    ax.get_xaxis().set_tick_params(direction='out', labelsize = 8)\n",
    "    ax.get_yaxis().set_tick_params(labelsize = 8)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.set_xticks(np.arange(1, len(labels) + 1))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlim(0.25, len(labels) + 0.75)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(3.3*1.2, 2.2*1.2*2), sharey=True)\n",
    "\n",
    "ax1.set_title('Full Dataset (2,430,981 Observations)', fontsize=10)\n",
    "ax1.set_ylabel('Prices (CNY Fen)', fontsize=8)\n",
    "parts = ax1.violinplot(\n",
    "        df_all, showmeans=False, showmedians=False,\n",
    "        showextrema=False)\n",
    "\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('royalblue')\n",
    "    pc.set_edgecolor('')\n",
    "    pc.set_alpha(0.75)\n",
    "\n",
    "ax2.set_title('Clicks Only Dataset (1,793 Observations)', fontsize=10)\n",
    "ax2.set_ylabel('Prices (CNY Fen)', fontsize=8)\n",
    "\n",
    "parts2 = ax2.violinplot(\n",
    "        df_clicks, showmeans=False, showmedians=False,\n",
    "        showextrema=False)\n",
    "\n",
    "for pc in parts2['bodies']:\n",
    "    pc.set_facecolor('darkred')\n",
    "    pc.set_edgecolor('')\n",
    "    pc.set_alpha(0.75)\n",
    "\n",
    "\n",
    "quartile1, medians, quartile3 = np.percentile(df_clicks, [25, 50, 75], axis=0)\n",
    "whiskers = np.array([\n",
    "    adjacent_values(sorted_array, q1, q3)\n",
    "    for sorted_array, q1, q3 in zip(df_all, quartile1, quartile3)])\n",
    "whiskersMin, whiskersMax = whiskers[:, 0], whiskers[:, 1]\n",
    "\n",
    "inds = np.arange(1, len(medians) + 1)\n",
    "ax2.scatter(inds, medians, marker='o', color='white', s=30, zorder=3)\n",
    "ax2.vlines(inds, quartile1, quartile3, color='k', linestyle='-', lw=5)\n",
    "ax2.vlines(inds, whiskersMin, whiskersMax, color='k', linestyle='-', lw=1)\n",
    "\n",
    "quartile1, medians, quartile3 = np.percentile(df_all, [25, 50, 75], axis=0)\n",
    "whiskers = np.array([\n",
    "    adjacent_values(sorted_array, q1, q3)\n",
    "    for sorted_array, q1, q3 in zip(df_all, quartile1, quartile3)])\n",
    "whiskersMin, whiskersMax = whiskers[:, 0], whiskers[:, 1]\n",
    "\n",
    "inds = np.arange(1, len(medians) + 1)\n",
    "ax1.scatter(inds, medians, marker='o', color='white', s=30, zorder=3)\n",
    "ax1.vlines(inds, quartile1, quartile3, color='k', linestyle='-', lw=5)\n",
    "ax1.vlines(inds, whiskersMin, whiskersMax, color='k', linestyle='-', lw=1)\n",
    "\n",
    "# Creating the style for axis.\n",
    "labels = ['Bid Price', 'Pay Price', 'Floor Price']\n",
    "for ax in [ax1, ax2]:\n",
    "    set_axis_style(ax, labels)\n",
    "\n",
    "plt.subplots_adjust(top=0.94, bottom=0.05, hspace=0.20)\n",
    "plt.show()\n",
    "plt.savefig(os.getcwd()+'/results/violinplot.pdf')\n",
    "\n",
    "# Establish the prise amount.\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "params = {'legend.fontsize': '6',\n",
    "          'figure.figsize': (3.3*1.2, 2.2*1.2),\n",
    "          'axes.labelsize': '8',\n",
    "          'axes.titlesize': '10',\n",
    "          'xtick.labelsize': '6',\n",
    "          'ytick.labelsize': '6'}\n",
    "sns.set(font_scale=6/8)\n",
    "\n",
    "\n",
    "# Related the prices.\n",
    "df_corr = train_dataset[['bidprice', 'payprice', 'slotprice']].corr()\n",
    "df_corr.columns = [['Bid Price', 'Pay Price', 'Floor Price']]\n",
    "corr = df_corr.corr().round(2)\n",
    "\n",
    "\n",
    "# create the traingle.\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(3.3*1.2, 2.2*1.2))\n",
    "plt.tick_params(labelsize=6)\n",
    "plt.title('Correlogram of Prices', fontsize=10)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(fontsize=6)\n",
    "plt.xticks(rotation=0)\n",
    "# Difference between the colormap\n",
    "sns.set_style(\"whitegrid\")\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# Draw the heat map ratio.\n",
    "sns.heatmap(corr,  cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.savefig(os.getcwd()+'/results/correlogram.pdf')\n",
    "# import the graphplot\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('seaborn-whitegrid')\n",
    "params = {'legend.fontsize': '6',\n",
    "          'figure.figsize': (3.3*1.2, 2.2*1.2),\n",
    "          'axes.labelsize': '8',\n",
    "          'axes.titlesize': '10',\n",
    "          'xtick.labelsize': '6',\n",
    "          'ytick.labelsize': '6'}\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(3.3*1.2, 2.2*3.6), sharey=False)\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "#Adding the three axis in the graph.\n",
    "fig = plt.figure(figsize=(3.3*1.2, 2.2*1.2))\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    ax1 = fig.add_subplot(111)\n",
    "CTR_vs_Weekday = train_dataset[['click', 'bidprice', 'payprice', 'advertiser', 'weekday']].groupby(['advertiser', 'weekday']).sum()\n",
    "CTR_vs_Weekday['impressions'] = train_dataset.groupby(['advertiser', 'weekday']).size()\n",
    "CTR_vs_Weekday['CTR'] = CTR_vs_Weekday['click'] / CTR_vs_Weekday['impressions']\n",
    "Plot_CTR_vs_Weekday = CTR_vs_Weekday.unstack('advertiser').loc[:, 'CTR'][[1458, 3358]]\n",
    "Plot_CTR_vs_Weekday.fillna(0.0, inplace=True)  \n",
    "# set the values at zero\n",
    "Plot_CTR_vs_Weekday.plot(kind=\"line\", color=['royalblue', 'darkred', 'darkgreen'], ax=ax1) \n",
    " # find out the check for column.\n",
    "ax1.set_title('CTR vs Weekday', fontsize=10)\n",
    "ax1.set_ylabel('CTR', fontsize=8)\n",
    "ax1.set_xlabel('Weekday', fontsize=8)\n",
    "# crt the operating system\n",
    "CTR_vs_OS = train_dataset[['click', 'bidprice', 'payprice', 'advertiser', 'opsys']].groupby(['advertiser', 'opsys']).sum()\n",
    "CTR_vs_OS['impressions'] = train_dataset.groupby(['advertiser', 'opsys']).size()\n",
    "CTR_vs_OS['CTR'] = CTR_vs_OS['click'] / CTR_vs_OS['impressions']\n",
    "Plot_CTR_vs_OS = CTR_vs_OS.unstack('advertiser').loc[:, 'CTR'][[1458, 3358]]\n",
    "Plot_CTR_vs_OS.fillna(0.0, inplace=True) \n",
    " # Fill Nans with zero\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "Plot_CTR_vs_OS.plot(kind=\"bar\", color=['royalblue', 'darkred', 'forestgreen', 'darkorange'], ax=ax2) \n",
    " # kind=\"bar\"\n",
    "ax2.set_title('CTR vs Operating System', fontsize=10)\n",
    "ax2.set_ylabel('CTR', fontsize=8)\n",
    "ax2.set_xlabel('Operating System', fontsize=8)\n",
    "ax2.xaxis.set_tick_params(rotation=0)\n",
    "\n",
    "# Ad Exchangeprocess\n",
    "CTR_vs_ad = train_dataset[['click', 'bidprice', 'payprice', 'advertiser', 'adExchangeprocess']].groupby(['advertiser', 'adExchangeprocess']).sum()\n",
    "CTR_vs_ad['impressions'] = train_dataset.groupby(['advertiser', 'adExchangeprocess']).size()\n",
    "CTR_vs_ad['CTR'] = CTR_vs_ad['click'] / CTR_vs_ad['impressions']\n",
    "Plot_CTR_vs_ad = CTR_vs_ad.unstack('advertiser').loc[:, 'CTR'][[1458, 3358]]\n",
    "Plot_CTR_vs_ad.fillna(0.0, inplace=True) \n",
    " # Fill Nans with zero\n",
    "Plot_CTR_vs_ad.plot(kind=\"bar\", color=['royalblue', 'darkred', 'forestgreen', 'darkorange'], ax=ax3) \n",
    " # check the format for column name.\n",
    "ax3.set_title('CTR vs Ad Exchangeprocess', fontsize=10)\n",
    "ax3.set_ylabel('CTR', fontsize=8)\n",
    "ax3.set_xlabel('Ad Exchangeprocess', fontsize=8)\n",
    "ax3.xaxis.set_tick_params(rotation=0)\n",
    "plt.subplots_adjust(top=0.96, bottom=0.05, hspace=0.30, left = 0.2)\n",
    "plt.show()\n",
    "plt.savefig(os.getcwd()+'/results/adv_exactone.pdf', dpi = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 ans\n",
    "#importing the all packages in your PC first.\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from pylab import rcParams\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from fastFM import als\n",
    "import scipy.sparse as sp\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import svm\n",
    "from sknn.mlp import Classifier, Layer\n",
    "import os\n",
    "\n",
    "# plot the ROC curve\n",
    "def plot_ROC_curve(data, prediction, model=None, minority_class=None):\n",
    "\n",
    "    # join  fpr, tpr, thresholds and roc auc\n",
    "    fpr, tpr, thresholds = roc_curve(data, prediction)\n",
    "    roc_auc = roc_auc_score(data, prediction)\n",
    "\n",
    "    if model != None:\n",
    "        label_title = '%s (AUC = %0.3f)' % (model, roc_auc)\n",
    "\n",
    "    else:\n",
    "        label_title = 'ROC Curve (AUC = %0.3f)' % roc_auc\n",
    "\n",
    "    if minority_class != None:\n",
    "        plot_title = 'ROC Curve (Sample with %.1f%% Minority Class)' %(minority_class*100)\n",
    "\n",
    "    else:\n",
    "        plot_title = 'ROC'\n",
    "\n",
    "    # ROS should be plotted.,\n",
    "    rcParams['figure.figsize'] = 3.3 * 1.2, 2.2 * 1.4\n",
    "    plt.tick_params(labelsize=6)\n",
    "    plt.plot(fpr, tpr, label=label_title)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate or (1 - Specifity)', fontsize=8)\n",
    "    plt.ylabel('True Positive Rate or (Sensitivity)', fontsize=8)\n",
    "    plt.title(plot_title, fontsize=10)\n",
    "    plt.legend(loc=\"lower right\", fontsize=6)\n",
    "\n",
    "\n",
    "# using Regression model.\n",
    "def logistic_model(train, validation,\n",
    "                   parameters = {'C': [0.1, 1, 2, 5, 10],\n",
    "                  'penalty': ['l1', 'l2'],\n",
    "                  'class_weight': ['unbalanced'],\n",
    "                  'solver': ['saga'],\n",
    "                  'tol': [0.01],\n",
    "                  'max_iter': [1]},\n",
    "                   use_gridsearch = 'yes',\n",
    "                   refit = 'yes',\n",
    "                   refit_iter = 100,\n",
    "                   use_saved_model = 'no',\n",
    "                   to_plot ='yes',\n",
    "                   random_seed = 500,\n",
    "                   save_model = 'yes'):\n",
    "\n",
    "    if use_gridsearch == 'yes':\n",
    "\n",
    "        print('Running gridsearch for hyperparameter tuning.')\n",
    "\n",
    "        # Create the model is must.\n",
    "        model = GridSearchCV(LogisticRegression(), parameters, cv=3, verbose=10, scoring = 'roc_auc')\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "        # Viwed the parameters\n",
    "        print('Best Penalty:', model.best_estimator_.get_params()['penalty'])\n",
    "        print('Best C:', model.best_estimator_.get_params()['C'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "         # using refit\n",
    "            model = LogisticRegression(C=model.best_estimator_.get_params()['C'],\n",
    "                                       penalty=model.best_estimator_.get_params()['penalty'],\n",
    "                                       solver=model.best_estimator_.get_params()['solver'],\n",
    "                                       class_weight=model.best_estimator_.get_params()['class_weight'],\n",
    "                                       max_iter=refit_iter,\n",
    "                                       n_jobs=model.best_estimator_.get_params()['n_jobs'],\n",
    "                                       tol=model.best_estimator_.get_params()['tol'],\n",
    "                                       random_state=random_seed,\n",
    "                                       verbose=10)\n",
    "\n",
    "            # Refit\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = model.best_estimator_.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    elif use_saved_model == 'yes':\n",
    "\n",
    "        # The files should be loade and saved.\n",
    "        model_filename = os.getcwd() + \"/models/logistic_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        # Viwed the parameters\n",
    "        print('Saved Model Penalty:', saved_model.get_params()['penalty'])\n",
    "        print('Saved Model C:', saved_model.get_params()['C'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = LogisticRegression(C=saved_model.get_params()['C'],\n",
    "                                       penalty=saved_model.get_params()['penalty'],\n",
    "                                       solver=saved_model.get_params()['solver'],\n",
    "                                       class_weight=saved_model.get_params()['class_weight'],\n",
    "                                       max_iter=refit_iter,\n",
    "                                       n_jobs=saved_model.get_params()['n_jobs'],\n",
    "                                       tol=saved_model.get_params()['tol'],\n",
    "                                       random_state=random_seed,\n",
    "                                       verbose=10)\n",
    "\n",
    "            # Model should be fitted\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "            model = saved_model\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = LogisticRegression(C=parameters['C'], penalty=parameters['penalty'], solver='saga',\n",
    "                                   class_weight = parameters['class_weight'],\n",
    "                                   max_iter = parameters['max_iter'],\n",
    "                                   n_jobs = parameters['n_jobs'],\n",
    "                                   tol=parameters['tol'],\n",
    "                                   random_state = random_seed,\n",
    "                                   verbose=10)\n",
    "\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for Logistic Model\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the logistic model to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/logistic_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]\n",
    "\n",
    "#precision recall\n",
    "\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "#features\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "#dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n",
    "                                                    test_size=.5,\n",
    "                                                    random_state=random_state)\n",
    "classifier = svm.LinearSVC(random_state=random_state)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_score = classifier.decision_function(X_test)\n",
    "\n",
    "#average precision calculation\n",
    "import average_precision_score\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "#calling precision again\n",
    "import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "\n",
    "import label_binarize\n",
    "\n",
    "#label binaries\n",
    "Y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = Y.shape[1]\n",
    "\n",
    "#divided train data sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "import OneVsRestClassifier\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_score = classifier.decision_function(X_test)\n",
    "import precision_recall_curve\n",
    "import average_precision_score\n",
    "\n",
    "#for each statement\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                        y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n",
    "\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "    y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall[\"micro\"], precision[\"micro\"], step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(average_precision[\"micro\"]))\n",
    "from itertools import cycle\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Using RandomForestClassifier\n",
    "def random_forest(train, validation,\n",
    "                   parameters = {'max_depth': [2,3,4,5,6,7,8,9,10,11,12, None],\n",
    "              'min_samples_split' :[4,5,6],\n",
    "              \"n_estimators\" : [10],\n",
    "              \"min_samples_leaf\": [1,2,3,4,5],\n",
    "              \"max_features\": [4,5,6,\"sqrt\"],\n",
    "              \"criterion\": ['gini','entropy'],\n",
    "                                 'random_state': 500},\n",
    "                   use_gridsearch = 'yes',\n",
    "                   refit = 'yes',\n",
    "                   refit_iter = 100,\n",
    "                   use_saved_model = 'no',\n",
    "                  save_model = 'no',\n",
    "                   to_plot ='yes',\n",
    "                   random_seed = 500):\n",
    "\n",
    "    if use_gridsearch == 'yes':\n",
    "\n",
    "        # Create the model is must.\n",
    "        model = GridSearchCV(RandomForestClassifier(), parameters, cv=3, verbose=10, scoring = 'roc_auc')\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "        # Viwed the parameters\n",
    "        print('Best Max Depth:', model.best_estimator_.get_params()['max_depth'])\n",
    "        print('Best Min Sample Split:', model.best_estimator_.get_params()['min_samples_split'])\n",
    "        print('Best Min Samples Leaf:', model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "        print('Best Max Features:', model.best_estimator_.get_params()['max_features'])\n",
    "        print('Best Criterion:', model.best_estimator_.get_params()['criterion'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = RandomForestClassifier(max_depth=model.best_estimator_.get_params()[\"max_depth\"]\n",
    "                                   , max_features=model.best_estimator_.get_params()['max_features']\n",
    "                                   , min_samples_leaf=model.best_estimator_.get_params()['min_samples_leaf']\n",
    "                                   , min_samples_split=model.best_estimator_.get_params()['min_samples_split']\n",
    "                                   , criterion=model.best_estimator_.get_params()['criterion']\n",
    "                                   , n_estimators=refit_iter\n",
    "                                   , n_jobs=3\n",
    "                                   , verbose=10\n",
    "                                           , random_state= random_seed)\n",
    "\n",
    "            # Refit\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = model.best_estimator_.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    elif use_saved_model == 'yes':\n",
    "\n",
    "        # The files should be loade and saved.\n",
    "        model_filename = os.getcwd() + \"/models/rf_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        # View saved model hyperparameters\n",
    "        print('Saved Model Max Depth:', saved_model.get_params()['max_depth'])\n",
    "        print('Saved Model Min Sample Split:', saved_model.get_params()['min_samples_split'])\n",
    "        print('Saved Model Min Samples Leaf:', saved_model.get_params()['min_samples_leaf'])\n",
    "        print('Saved Model Max Features:', saved_model.get_params()['max_features'])\n",
    "        print('Saved Model Criterion:', saved_model.get_params()['criterion'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = RandomForestClassifier(max_depth=saved_model.get_params()[\"max_depth\"]\n",
    "                                   , max_features=saved_model.get_params()['max_features']\n",
    "                                   , min_samples_leaf=saved_model.get_params()['min_samples_leaf']\n",
    "                                   , min_samples_split=saved_model.get_params()['min_samples_split']\n",
    "                                   , criterion=saved_model.get_params()['criterion']\n",
    "                                   , n_estimators=refit_iter\n",
    "                                   , n_jobs=3\n",
    "                                   , verbose=10\n",
    "                                           , random_state=random_seed)\n",
    "            # Model should be fitted\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "            model = saved_model\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = RandomForestClassifier(max_depth=parameters[\"max_depth\"]\n",
    "                                       , max_features=parameters['max_features']\n",
    "                                       , min_samples_leaf=parameters['min_samples_leaf']\n",
    "                                       , min_samples_split=parameters['min_samples_split']\n",
    "                                       , criterion=parameters['criterion']\n",
    "                                       , n_estimators=parameters['n_estimators']\n",
    "                                       , n_jobs=3\n",
    "                                       , verbose=10\n",
    "                                       , random_state=random_seed)\n",
    "\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for Random Forest Model\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the random forest model to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/rf_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]\n",
    "\n",
    "\n",
    "# RandomForestClassifier extreme\n",
    "def extreme_random_forest(train, validation,\n",
    "                   parameters = {'max_depth': [2,3,4,5,6,7,8,9,10,11,12, None],\n",
    "              'min_samples_split' :[4,5,6],\n",
    "              \"n_estimators\" : [10],\n",
    "              \"min_samples_leaf\": [1,2,3,4,5],\n",
    "              \"max_features\": [4,5,6,\"sqrt\"],\n",
    "              \"criterion\": ['gini','entropy']},\n",
    "                   use_gridsearch = 'yes',\n",
    "                   refit = 'yes',\n",
    "                   refit_iter = 100,\n",
    "                   use_saved_model = 'no',\n",
    "                          save_model = 'yes',\n",
    "                   to_plot ='yes',\n",
    "                   random_seed = 500):\n",
    "\n",
    "\n",
    "    if use_gridsearch == 'yes':\n",
    "\n",
    "        # Create the model is must.\n",
    "        model = GridSearchCV(ExtraTreesClassifier(), parameters, cv=3, verbose=10, scoring = 'roc_auc')\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "        # Viwed the parameters\n",
    "        print('Best Max Depth:', model.best_estimator_.get_params()['max_depth'])\n",
    "        print('Best Min Sample Split:', model.best_estimator_.get_params()['min_samples_split'])\n",
    "        print('Best Min Samples Leaf:', model.best_estimator_.get_params()['min_samples_leaf'])\n",
    "        print('Best Max Features:', model.best_estimator_.get_params()['max_features'])\n",
    "        print('Best Criterion:', model.best_estimator_.get_params()['criterion'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = ExtraTreesClassifier(max_depth=model.best_estimator_.get_params()[\"max_depth\"]\n",
    "                                   , max_features=model.best_estimator_.get_params()['max_features']\n",
    "                                   , min_samples_leaf=model.best_estimator_.get_params()['min_samples_leaf']\n",
    "                                   , min_samples_split=model.best_estimator_.get_params()['min_samples_split']\n",
    "                                   , criterion=model.best_estimator_.get_params()['criterion']\n",
    "                                   , n_estimators=refit_iter\n",
    "                                   , n_jobs=3\n",
    "                                   , verbose=10\n",
    "                                         , random_state = random_seed)\n",
    "\n",
    "            # Refit\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = model.best_estimator_.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    elif use_saved_model == 'yes':\n",
    "\n",
    "        # The files should be loade and saved.\n",
    "        model_filename = os.getcwd() + \"/models/erf_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        # View saved model hyperparameters\n",
    "        print('Saved Model Max Depth:', saved_model.get_params()['max_depth'])\n",
    "        print('Saved Model Min Sample Split:', saved_model.get_params()['min_samples_split'])\n",
    "        print('Saved Model Min Samples Leaf:', saved_model.get_params()['min_samples_leaf'])\n",
    "        print('Saved Model Max Features:', saved_model.get_params()['max_features'])\n",
    "        print('Saved Model Criterion:', saved_model.get_params()['criterion'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = ExtraTreesClassifier(max_depth=saved_model.get_params()[\"max_depth\"]\n",
    "                                   , max_features=saved_model.get_params()['max_features']\n",
    "                                   , min_samples_leaf=saved_model.get_params()['min_samples_leaf']\n",
    "                                   , min_samples_split=saved_model.get_params()['min_samples_split']\n",
    "                                   , criterion=saved_model.get_params()['criterion']\n",
    "                                   , n_estimators=refit_iter\n",
    "                                   , n_jobs=3\n",
    "                                   , verbose=10\n",
    "                                         , random_state=random_seed)\n",
    "            # Model should be fitted\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "            model = saved_model\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = ExtraTreesClassifier(max_depth=parameters[\"max_depth\"]\n",
    "                                       , max_features=parameters['max_features']\n",
    "                                       , min_samples_leaf=parameters['min_samples_leaf']\n",
    "                                       , min_samples_split=parameters['min_samples_split']\n",
    "                                       , criterion=parameters['criterion']\n",
    "                                       , n_estimators=parameters['n_estimators']\n",
    "                                       , n_jobs=3\n",
    "                                       , verbose=10\n",
    "                                     , random_state=random_seed)\n",
    "\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the extreme random forest model to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/erf_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for Extreme Random Forest Model\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]\n",
    "\n",
    "\n",
    "# XGBClassifier algorithm\n",
    "def gradient_boosted_trees(train, validation,\n",
    "                           parameters={'max_depth': [15, 20],\n",
    "                                       \"n_estimators\": [10],\n",
    "                                       \"learning_rate\": [0.05, 0.1],\n",
    "                                       \"colsample_bytrees\": [0.5],\n",
    "                                       \"reg_alpha\": [0.1],\n",
    "                                       \"reg_lambda\": [0.1],\n",
    "                                       \"subsample\": [1],\n",
    "                                       \"gamma\": [0.1]},\n",
    "                   use_gridsearch = 'yes',\n",
    "                   refit = 'yes',\n",
    "                   refit_iter = 20,\n",
    "                   use_saved_model = 'no',\n",
    "                   save_model = 'no',\n",
    "                   to_plot ='yes',\n",
    "                   random_seed = 500):\n",
    "\n",
    "\n",
    "    if use_gridsearch == 'yes':\n",
    "\n",
    "        # Create the model is must.\n",
    "        model = GridSearchCV(xgboost.XGBClassifier(), parameters, cv=3, verbose=10, scoring = 'roc_auc')\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "        # Viwed the parameters\n",
    "        print('Saved Model Max Depth:', model.best_estimator_.get_params()['max_depth'])\n",
    "        print('Saved Model Learning Rate:', model.best_estimator_.get_params()['learning_rate'])\n",
    "        print('Saved Model Sub Sample:', model.best_estimator_.get_params()['subsample'])\n",
    "        print('Saved Model Colsample By Trees:', model.best_estimator_.get_params()['colsample_bytree'])\n",
    "        print('Saved Model Reg Alpha:', model.best_estimator_.get_params()['reg_alpha'])\n",
    "        print('Saved Model Lambda:', model.best_estimator_.get_params()['reg_lambda'])\n",
    "        print('Saved Model Gamma:', model.best_estimator_.get_params()['gamma'])\n",
    "\n",
    "\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = xgboost.XGBClassifier(max_depth=model.best_estimator_.get_params()['max_depth']\n",
    "                                          , learning_rate=model.best_estimator_.get_params()['learning_rate']\n",
    "                                          , subsample=model.best_estimator_.get_params()['subsample']\n",
    "                                          , colsample_bytree=model.best_estimator_.get_params()['colsample_bytree']\n",
    "                                          , reg_alpha=model.best_estimator_.get_params()['reg_alpha']\n",
    "                                          , reg_lambda=model.best_estimator_.get_params()['reg_lambda']\n",
    "                                          , gamma=model.best_estimator_.get_params()['gamma']\n",
    "                                          , n_estimators=refit_iter\n",
    "                                          , n_jobs=3\n",
    "                                          , verbose=10\n",
    "                                          , random_state = random_seed\n",
    "                                          , silent=False)\n",
    "\n",
    "            # Refit\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = model.best_estimator_.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    elif use_saved_model == 'yes':\n",
    "\n",
    "        # The files should be loade and saved.\n",
    "        model_filename = os.getcwd() + \"/models/xgb_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        # View saved model hyperparameters\n",
    "        print('Saved Model Max Depth:', saved_model.get_params()['max_depth'])\n",
    "        print('Saved Model Learning Rate:', saved_model.get_params()['learning_rate'])\n",
    "        print('Saved Model Sub Sample:', saved_model.get_params()['subsample'])\n",
    "        print('Saved Model Colsample By Trees:', saved_model.get_params()['colsample_bytree'])\n",
    "        print('Saved Model Reg Alpha:', saved_model.get_params()['reg_alpha'])\n",
    "        print('Saved Model Lambda:', saved_model.get_params()['reg_lambda'])\n",
    "        print('Saved Model Gamma:', saved_model.get_params()['gamma'])\n",
    "\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = xgboost.XGBClassifier(max_depth=saved_model.get_params()['max_depth']\n",
    "                                   , learning_rate=saved_model.get_params()['learning_rate']\n",
    "                                   , subsample=saved_model.get_params()['subsample']\n",
    "                                          , colsample_bytree=saved_model.get_params()['colsample_bytree']\n",
    "                                          , reg_alpha=saved_model.get_params()['reg_alpha']\n",
    "                                          , reg_lambda=saved_model.get_params()['reg_lambda']\n",
    "                                          , gamma=saved_model.get_params()['gamma']\n",
    "                                          , n_estimators=refit_iter\n",
    "                                   , n_jobs=3\n",
    "                                   , verbose=10\n",
    "                                          , random_state=random_seed,\n",
    "                                          silent=False)\n",
    "            # Model should be fitted\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "            model = saved_model\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = xgboost.XGBClassifier(max_depth=parameters['max_depth']\n",
    "                                      , learning_rate=parameters['learning_rate']\n",
    "                                      , subsample=parameters['subsample']\n",
    "                                      , colsample_bytree=parameters['colsample_bytree']\n",
    "                                      , reg_alpha=parameters['reg_alpha']\n",
    "                                      , reg_lambda=parameters['reg_lambda']\n",
    "                                      , gamma=parameters['gamma']\n",
    "                                      , n_estimators=refit_iter\n",
    "                                      , n_jobs=3\n",
    "                                      , verbose=10\n",
    "                                      , random_state=random_seed\n",
    "                                      , silent=False)\n",
    "\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for XGBoost Model\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the gradient boosted tree model to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/xgb_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]\n",
    "\n",
    "\n",
    "# Using SVM for prediction\n",
    "def support_vector_machine(train, validation,\n",
    "                           parameters={'C': [0.1, 1, 2],\n",
    "                                       \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                                       \"degree\": [2, 3, 4],\n",
    "                                       \"gamma\": ['auto'],\n",
    "                                       \"tol\": [0.001],\n",
    "                                       \"max_iter\": [10],\n",
    "                                       \"probability\": [True],\n",
    "                                       \"cache_size\": [1000]},\n",
    "                   use_gridsearch = 'yes',\n",
    "                   refit = 'yes',\n",
    "                   refit_iter = 20,\n",
    "                   use_saved_model = 'no',\n",
    "                   save_model = 'yes',\n",
    "                   to_plot ='yes',\n",
    "                   random_seed = 500):\n",
    "\n",
    "\n",
    "    if use_gridsearch == 'yes':\n",
    "\n",
    "        # Create the model is must.\n",
    "        model = GridSearchCV(svm.SVC(), parameters, cv=3, verbose=10, scoring = 'roc_auc')\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "        # Viwed the parameters\n",
    "        print('Saved Model C:', model.best_estimator_.get_params()['C'])\n",
    "        print('Saved Kernel:', model.best_estimator_.get_params()['kernel'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = svm.SVC(C=model.best_estimator_.get_params()['C']\n",
    "                                          , kernel=model.best_estimator_.get_params()['kernel']\n",
    "                                          , degree=model.best_estimator_.get_params()['degree']\n",
    "                                          , gamma=model.best_estimator_.get_params()['gamma']\n",
    "                                          , tol=model.best_estimator_.get_params()['tol']\n",
    "                                          , max_iter=refit_iter\n",
    "                                          , verbose=10\n",
    "                                , random_state = random_seed\n",
    "                            ,probability=True)\n",
    "\n",
    "            # Refit\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = model.best_estimator_.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    elif use_saved_model == 'yes':\n",
    "\n",
    "        # The files should be loade and saved.\n",
    "        model_filename = os.getcwd() + \"/models/svm_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        # View saved model hyperparameters\n",
    "        print('Saved Model C:', saved_model.get_params()['C'])\n",
    "        print('Saved Kernel:', saved_model.get_params()['kernel'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = svm.SVC(C=saved_model.get_params()['C']\n",
    "                                          , kernel=saved_model.get_params()['kernel']\n",
    "                                          , degree=saved_model.get_params()['degree']\n",
    "                                          , gamma=saved_model.get_params()['gamma']\n",
    "                                          , tol=saved_model.get_params()['tol']\n",
    "                                          , max_iter=refit_iter\n",
    "                                          , verbose=10\n",
    "                            ,random_state = random_seed\n",
    "                            ,probability=True)\n",
    "            # Model should be fitted\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "            model = saved_model\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = svm.SVC(C=parameters['C']\n",
    "                                      , kernel=parameters['kernel']\n",
    "                                      , degree=parameters['degree']\n",
    "                                      , gamma=parameters['gamma']\n",
    "                                      , tol=parameters['tol']\n",
    "                                      , max_iter=refit_iter\n",
    "                                      , verbose=10\n",
    "                        , random_state=random_seed\n",
    "                        , probability=True)\n",
    "\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for SVM Model\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the support vector machines to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/svm_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]\n",
    "\n",
    "\n",
    "# using NAIVE BAYES for prediction\n",
    " def naive_bayes(train, validation, use_saved_model='yes', save_model='yes', to_plot ='yes'):\n",
    "\n",
    "    if use_saved_model == 'yes':\n",
    "\n",
    "        # The files should be loade and saved.\n",
    "        model_filename = os.getcwd() + \"/models/nb_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        # prediction should be maked\n",
    "        prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = GaussianNB()\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "        # prediction should be maked\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for Naive Bayes.\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the Naive Bayes model to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/nb_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]\n",
    "\n",
    "# using the classification algorithm (KNN)\n",
    "def KNN(train, validation,\n",
    "                           parameters={'n_neighbors': [1, 2,3],\n",
    "                                       \"algorithm\": ['auto']},\n",
    "                   use_gridsearch = 'yes',\n",
    "                   refit = 'yes',\n",
    "                   use_saved_model = 'no',\n",
    "                   saved_model = [],\n",
    "                   to_plot ='yes',\n",
    "                   random_seed = 500):\n",
    "\n",
    "\n",
    "    if use_gridsearch == 'yes':\n",
    "\n",
    "        # Create the model is must.\n",
    "        model = GridSearchCV(KNeighborsClassifier(), parameters, cv=3, verbose=10, scoring = 'roc_auc')\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "        # Viwed the parameters\n",
    "        print('Best Model N Neighbours:', model.best_estimator_.get_params()['n_neighbors'])\n",
    "        print('Best Algorithm:', model.best_estimator_.get_params()['algorithm'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = KNeighborsClassifier(n_neighbors=model.best_estimator_.get_params()['n_neighbors']\n",
    "                                          , algorithm=model.best_estimator_.get_params()['algorithm'],\n",
    "                                         n_jobs = 3\n",
    "                                          , verbose=10)\n",
    "\n",
    "            # Refit\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = model.best_estimator_.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    elif use_saved_model == 'yes':\n",
    "\n",
    "        # View saved model hyperparameters\n",
    "        print('Saved Model N Neighbours:', saved_model.get_params()['n_neighbors'])\n",
    "        print('Saved Algorithm:', saved_model.get_params()['algorithm'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = KNeighborsClassifier(n_neighbors=saved_model.get_params()['n_neighbors']\n",
    "                                         , algorithm=saved_model.get_params()['algorithm'],\n",
    "                                         n_jobs=3\n",
    "                                         , verbose=10)\n",
    "            # Model should be fitted\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1), train['click'])\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "            model = saved_model\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = KNeighborsClassifier(n_neighbors=parameters['n_neighbors']\n",
    "                                     , algorithm=parameters['algorithm'],\n",
    "                                     n_jobs=3\n",
    "                                     , verbose=10)\n",
    "\n",
    "\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1), train['click'])\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1))\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for KNN Model\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]\n",
    "\n",
    "\n",
    "# (FIELD-AWARE FACTORIZATION MACHINE)\n",
    "def factorization_machine(train, validation,\n",
    "                           parameters={'init_stdev': [0.1],\n",
    "                                       \"rank\": [2, 3, 4],\n",
    "                                       'l2_reg_w': [0.1, 1, 2],\n",
    "                                       'l2_reg_V':[0.1, 0.5, 1],\n",
    "                                       'n_iter': [10]},\n",
    "                  # use_gridsearch = 'yes',\n",
    "                   refit = 'yes',\n",
    "                          refit_iter = 20,\n",
    "                   use_saved_model = 'no',\n",
    "                   save_model = 'yes',\n",
    "                   to_plot ='yes',\n",
    "                   random_seed = 500):\n",
    "\n",
    "    # Transform the data to sparse representation\n",
    "    train_X = train.drop(['click', 'bidprice', 'payprice'], axis=1)\n",
    "    sparse_train_X = sp.csc_matrix(train_X)\n",
    "    train_Y = train['click']\n",
    "    train_Y[train_Y == 0] = -1\n",
    "\n",
    "    validation_X = validation.drop(['click', 'bidprice', 'payprice'], axis=1)\n",
    "    validation_Y = validation['click']\n",
    "    validation_Y[validation_Y == 0] = -1\n",
    "    sparse_validation_X = sp.csc_matrix(validation_X)\n",
    "\n",
    "    if use_saved_model == 'yes':\n",
    "\n",
    "\n",
    "        model_filename = os.getcwd() + \"/models/fm_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        # View saved model hyperparameters\n",
    "        print('Saved Model Rank:', saved_model.get_params()['rank'])\n",
    "        print('Saved Model L2 Regularisation Parameter W:', saved_model.get_params()['l2_reg_w'])\n",
    "        print('Saved Model L2 Regularisation Parameter V:', saved_model.get_params()['l2_reg_V'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = als.FMClassification(rank=saved_model.get_params()['rank']\n",
    "                                          , l2_reg_w=saved_model.get_params()['l2_reg_w']\n",
    "                                         , l2_reg_V = saved_model.get_params()['l2_reg_V'],\n",
    "                                         n_iter = refit_iter,\n",
    "                                         random_state = random_seed)\n",
    "\n",
    "            model = model.fit(sparse_train_X, train_Y)\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(sparse_validation_X)\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(sparse_validation_X)\n",
    "            model = saved_model\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = als.FMClassification(rank=parameters['rank']\n",
    "                                     , l2_reg_w=parameters['l2_reg_w']\n",
    "                                     , l2_reg_V=parameters['l2_reg_V'],\n",
    "                                     n_iter=parameters['n_iter'],\n",
    "                                     random_state=random_seed)\n",
    "\n",
    "        model = model.fit(sparse_train_X, train_Y)\n",
    "        prediction = model.predict_proba(sparse_validation_X)\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for Factorization Machine Model\"% (roc_auc_score(validation_Y, prediction)))\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the Factorization Machine model to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/fm_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation_Y, prediction)\n",
    "\n",
    "    return model, prediction\n",
    "\n",
    "\n",
    "# Neural Network must be used.\n",
    "def neural_network(train, validation,\n",
    "                           parameters={'learning_rate': [0.005, 0.01],\n",
    "                                       \"learning_momentum\": ['0.9'],\n",
    "                                       \"regularize\": ['L2'],\n",
    "                                       \"dropout_rate\": [0.2],\n",
    "                                       \"batch_size\": [1],\n",
    "                                       \"n_stable\": [10],\n",
    "                                       \"n_iter\": [10],\n",
    "                                       'hidden0__units': [32, 64],\n",
    "                                       'hidden0__type': [\"Rectifier\"]},\n",
    "                   use_gridsearch='yes',\n",
    "                   refit='yes',\n",
    "                   refit_iter=20,\n",
    "                   use_saved_model='no',\n",
    "                   save_model='yes',\n",
    "                   to_plot='yes',\n",
    "                   random_seed=500):\n",
    "\n",
    "    if use_gridsearch == 'yes':\n",
    "\n",
    "        # Create layers object\n",
    "        nn_layers = [Layer(\"Rectifier\", units=64), Layer(\"Softmax\")]\n",
    "\n",
    "        # Create the model is must.\n",
    "        model = GridSearchCV(sknn.mlp.Classifier(layers=nn_layers, random_state=random_seed),\n",
    "                             parameters, cv=3, verbose=10, scoring='roc_auc')\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1).values, train['click'].values)\n",
    "\n",
    "        # Viwed the parameters\n",
    "        print('Saved Model Learning Rate:', model.best_estimator_.get_params()['learning_rate'])\n",
    "        print('Saved Model Dropout Rate:', model.best_estimator_.get_params()['dropout_rate'])\n",
    "        print('Saved Model Batch Size:', model.best_estimator_.get_params()['batch_size'])\n",
    "        print('Saved Model Hidden Architecture:', model.best_estimator_.get_params()['layers'])\n",
    "        print('Saved Model Learning Momentum:', model.best_estimator_.get_params()['learning_momentum'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = sknn.mlp.Classifier(layers=model.best_estimator_.get_params()['layers']\n",
    "                                          , learning_rate=model.best_estimator_.get_params()['learning_rate']\n",
    "                                        , learning_momentum=model.best_estimator_.get_params()['learning_momentum']\n",
    "                                        , dropout_rate=model.best_estimator_.get_params()['dropout_rate']\n",
    "                                          , batch_size=model.best_estimator_.get_params()['batch_size']\n",
    "                                          , n_iter=refit_iter\n",
    "                                          , verbose=10\n",
    "                                , random_state = random_seed)\n",
    "\n",
    "            # Refit\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1).values, train['click'].values)\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1).values)\n",
    "\n",
    "        else:\n",
    "            prediction = model.best_estimator_.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1).values)\n",
    "\n",
    "    elif use_saved_model == 'yes':\n",
    "\n",
    "        # The files should be loade and saved.\n",
    "        model_filename = os.getcwd() + \"/models/nn_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        # View saved model hyperparameters\n",
    "        print('Saved Model Learning Rate:', saved_model.get_params()['learning_rate'])\n",
    "        print('Saved Model Dropout Rate:', saved_model.get_params()['dropout_rate'])\n",
    "        print('Saved Model Batch Size:', saved_model.get_params()['batch_size'])\n",
    "        print('Saved Model Hidden Architecture:', saved_model.get_params()['layers'])\n",
    "        print('Saved Model Learning Momentum:', saved_model.get_params()['learning_momentum'])\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = sknn.mlp.Classifier(layers=saved_model.get_params()['layers']\n",
    "                                          , learning_rate=saved_model.get_params()['learning_rate']\n",
    "                                        , learning_momentum=saved_model.get_params()['learning_momentum']\n",
    "                                        , dropout_rate=saved_model.get_params()['dropout_rate']\n",
    "                                          , batch_size=saved_model.get_params()['batch_size']\n",
    "                                          , n_iter=refit_iter\n",
    "                                          , verbose=10\n",
    "                                , random_state = random_seed)\n",
    "\n",
    "            # Model should be fitted\n",
    "            model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1).values, train['click'].values)\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1).values)\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1).values)\n",
    "            model = saved_model\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Model should be fitted\n",
    "        model = sknn.mlp.Classifier(layers=parameters['layers']\n",
    "                                    , learning_rate=parameters['learning_rate']\n",
    "                                    , learning_momentum=parameters['learning_momentum']\n",
    "                                    , dropout_rate=parameters['dropout_rate']\n",
    "                                    , batch_size=parameters['batch_size']\n",
    "                                    , n_iter=refit_iter\n",
    "                                    , verbose=10\n",
    "                                    , random_state=random_seed)\n",
    "\n",
    "        model = model.fit(train.drop(['click','bidprice', 'payprice'], axis=1).values, train['click'].values)\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1).values)\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for Neural Network Model\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the neural network to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/nn_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "pca = PCA(n_components=2)\n",
    "kind = ['regular', 'borderline1', 'borderline2', 'svm']\n",
    "sm = [SMOTE(kind=k) for k in kind]\n",
    "X_resampled = []\n",
    "y_resampled = []\n",
    "X_res_vis = []\n",
    "for method in sm:\n",
    "    X_res, y_res = method.fit_sample(X, y)\n",
    "    X_resampled.append(X_res)\n",
    "    y_resampled.append(y_res)\n",
    "    X_res_vis.append(pca.transform(X_res))\n",
    "\n",
    "# Creating the Stack classifier\n",
    "def stacking_classifier(train, validation, refit = 'yes', use_saved_model = 'no', save_model = 'yes', to_plot ='yes',\n",
    "                        meta_leaner_parameters={'max_depth':20, \"n_estimators\":20, \"learning_rate\":0.05,\n",
    "                                                'silent':False, 'n_jobs':3,'subsample':1, 'objective':'binary:logistic',\n",
    "                                                'colsample_bytree':1, 'eval_metric':\"auc\", 'reg_alpha':0.1,\n",
    "                                                'reg_lambda':0.1, 'random_state':500},\n",
    "                        stacking_cv_parameters={'use_probas': False,\n",
    "                                               'use_features_in_secondary': True,\n",
    "                                               'cv': 5,\n",
    "                                               'store_train_meta_features': True,\n",
    "                                               'refit': True}):\n",
    "\n",
    "    if use_saved_model == 'no':\n",
    "\n",
    "        # Import all the grid searched models\n",
    "\n",
    "        # Logistic\n",
    "        model_filename = os.getcwd() + \"/models/logistic_model.pkl\"\n",
    "        log_model = joblib.load(model_filename)\n",
    "\n",
    "        # Random Forest\n",
    "        model_filename = os.getcwd() + \"/models/rf_model.pkl\"\n",
    "        rf_model = joblib.load(model_filename)\n",
    "\n",
    "        # Extreme Random Forest\n",
    "        model_filename = os.getcwd() + \"/models/erf_model.pkl\"\n",
    "        erf_model = joblib.load(model_filename)\n",
    "\n",
    "        # XGBoost\n",
    "        model_filename = os.getcwd() + \"/models/xgb_model.pkl\"\n",
    "        xgb_model = joblib.load(model_filename)\n",
    "\n",
    "        # SVM\n",
    "        model_filename = os.getcwd() + \"/models/svm_model.pkl\"\n",
    "        svm_model = joblib.load(model_filename)\n",
    "\n",
    "        # Naive Bayes\n",
    "        model_filename = os.getcwd() + \"/models/nb_model.pkl\"\n",
    "        nb_model = joblib.load(model_filename)\n",
    "\n",
    "        # Neural Network\n",
    "        model_filename = os.getcwd() + \"/models/nn_model.pkl\"\n",
    "        nn_model = joblib.load(model_filename)\n",
    "\n",
    "        meta_learner = xgboost.XGBClassifier(max_depth=meta_leaner_parameters['max_depth'],\n",
    "                                             n_estimators=meta_leaner_parameters['n_estimators'],\n",
    "                                             learning_rate=meta_leaner_parameters['learning_rate'],\n",
    "                                             silent=meta_leaner_parameters['silent'],\n",
    "                                             n_jobs=meta_leaner_parameters['n_jobs'],\n",
    "                                             subsample=meta_leaner_parameters['subsample'],\n",
    "                                             objective=meta_leaner_parameters['objective'],\n",
    "                                             colsample_bytree=meta_leaner_parameters['colsample_bytree'],\n",
    "                                             eval_metric=meta_leaner_parameters['eval_metric'],\n",
    "                                             reg_alpha=meta_leaner_parameters['reg_alpha'],\n",
    "                                             reg_lambda=meta_leaner_parameters['reg_lambda'],\n",
    "                                             random_state = meta_leaner_parameters['random_state'])\n",
    "\n",
    "        model = StackingCVClassifier(classifiers=[rf_model, erf_model, xgb_model],\n",
    "                                    meta_classifier=meta_learner, use_probas=stacking_cv_parameters['use_probas'],\n",
    "                                    use_features_in_secondary = stacking_cv_parameters['use_features_in_secondary'],\n",
    "                                    store_train_meta_features=stacking_cv_parameters['store_train_meta_features'],\n",
    "                                    cv = stacking_cv_parameters['cv'])\n",
    "\n",
    "        model = model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1).values, train['click'].values)\n",
    "        prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1).values)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # The files should be loade and saved.\n",
    "        model_filename = os.getcwd() + \"/models/stacked_model.pkl\"\n",
    "        saved_model = joblib.load(model_filename)\n",
    "\n",
    "        if refit == 'yes':\n",
    "\n",
    "            # using refit run for the below command\n",
    "            model = saved_model.fit(train.drop(['click', 'bidprice', 'payprice'], axis=1).values, train['click'].values)\n",
    "\n",
    "            # prediction should be maked\n",
    "            prediction = model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1).values)\n",
    "\n",
    "        else:\n",
    "            prediction = saved_model.predict_proba(validation.drop(['click', 'bidprice', 'payprice'], axis=1).values)\n",
    "            model = saved_model\n",
    "\n",
    "\n",
    "    # Model should be changed\n",
    "    if save_model == 'yes':\n",
    "\n",
    "        print('Saving the stacked model to the disc.')\n",
    "        model_filename = os.getcwd() + \"/models/stacked_model.pkl\"\n",
    "        joblib.dump(model, model_filename, compress=9)\n",
    "\n",
    "\n",
    "    # scores should be printed\n",
    "    print(\"AUC: %0.5f for Stacking Model\"% (roc_auc_score(validation['click'], prediction[:, 1])))\n",
    "\n",
    "    if to_plot == 'yes':\n",
    "\n",
    "        plot_ROC_curve(validation['click'], prediction[:, 1])\n",
    "\n",
    "    return model, prediction[:,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
